# Planning steps for deep learning assignment
_Exported on 9/22/2025 at 12:22:41 GMT-3 from Cursor (1.5.11)_

---

**User**

Below is my undergraduate computer science deep learning class assignment. We need to work with diffusion models. We are only planning what we need to do in this assignment, so this stage is only concerned with planning. 

Therefore, considering the assignment below, plan the steps we must take to achieve a perfect score in this task. I am mostly unsure about where can we use such models, the number of generated images and the prompt itself. So be mindful of that and plan the tasks needs.

```
### Generative models and evaluation metrics

Evaluating the quality of images generated by generative models is not a trivial task. Unlike supervised tasks that allow metrics directly comparing the target variable, evaluating generated images usually involves using indirect metrics.

You must work individually or in pairs and develop a practical experiment involving image generative models and evaluation metrics. This assignment is divided into three stages:

### 1. Using a diffusion model
- Define a set of prompts to be used as input for the generative models (it is suggested to use two to four different prompts).
- Define the number of images to be generated for each prompt.
- Use a generative model to generate synthetic images.
- It is recommended to use ready-made APIs for image generation with these models or specific PyTorch libraries (for example, `diffusers`) to speed up this stage.

### 2. Image quality evaluation
- Apply at least two quantitative metrics to evaluate the quality of the generated images. Below are some suggestions:
  - Fréchet Inception Distance (FID)
  - Inception Score (IS)
  - Structural Similarity Index Measure (SSIM)
  - Perceptual Path Length (PPL)
- Compare the results obtained using the generative model with the metrics computed on real data.
- Discuss the limitations and advantages of the chosen metrics.

### 3. Presentation of results
- The presentation should be between 10 and 15 minutes.
- The code used to generate and evaluate the images must be submitted via Moodle.
- The material used during the presentation (ppt or similar) must be submitted on Moodle.
- The presentation must include:
  - An introduction to the diffusion models used.
  - A description of the prompts and the model hyperparameterization.
  - A detailed explanation of the chosen metrics.
  - An analysis of the results using the chosen metrics.
  - A discussion of the challenges and limitations of evaluating the quality of generated images by the group.
```

---

**Cursor**

### High-level plan to ace the assignment

- **use-case selection (where to use diffusion models)**
  - **data augmentation for vision tasks**: generate new training images for specific classes (clear, measurable impact).
  - **class-conditional synthesis**: generate clean “catalog-style” photos for specific object categories.
  - Choose a domain with good public real data to compare against; avoid faces for ethics and matching difficulties.

### 1) Model, dataset, prompts, and counts

- **model choice**
  - Prefer local, reproducible, controllable: `diffusers` with `runwayml/stable-diffusion-v1-5` or `stabilityai/stable-diffusion-2-1`.
  - If compute is tight, use an API (Replicate or Stability) but keep seeds and prompts logged.

- **real dataset (for comparisons)**
  - Pick an object-centric dataset with many examples and easy filtering:
    - Option A (recommended): COCO 2017 subsets for chosen categories.
    - Option B (compact): Oxford 102 Flowers (choose 4 classes).
    - Option C (simple high-res objects): Food-101 (choose 4 classes).
  - Ensure at least as many real images per prompt as generated images.

- **prompt set (4 prompts, object-centric, photo-real)**
  - For COCO-style objects:
    - “a high-resolution photo of a red rose on a plain background, studio lighting, 50mm lens”
    - “a high-resolution photo of a sunflower in a field, golden hour lighting, detailed petals”
    - “a high-resolution photo of a pepperoni pizza on a wooden table, overhead shot”
    - “a high-resolution photo of a yellow school bus on a street, daylight, realistic shadows”
  - Keep style modifiers consistent across prompts; optionally add a single negative prompt: “blurry, low-resolution, text, watermark”.

- **number of images per prompt**
  - Target for stable FID/IS within class constraints:
    - Minimum: 100 per prompt (total 400) — quick baseline.
    - Target: 250 per prompt (total 1,000) — balanced quality/stability.
    - Stretch: 500 per prompt (total 2,000) — best stability if time allows.
  - Match the same counts in real data per prompt. Keep image size fixed at 512x512.

- **generation settings (consistent across prompts)**
  - Sampler: `Euler a` or `DDIM` (any `diffusers` default is fine).
  - Steps: 30.
  - Guidance scale: 7.5 (SD 1.5) or 7.0 (SD 2.1).
  - Resolution: 512x512.
  - Seeds: sequential seeds (0..N-1) for diversity and reproducibility.
  - One negative prompt shared across all prompts, as above.

### 2) Evaluation design

- **metrics to compute (use at least two; we’ll do three for robustness)**
  - **FID**: distributional quality vs real images per prompt and overall.
  - **Inception Score (IS)**: confidence/diversity on generated images; also compute IS on matched real images as an upper bound.
  - **SSIM (optional but recommended)**: pair each generated image with its nearest neighbor from the real set using Inception or CLIP embeddings; report mean SSIM of these pairs.

- **comparisons and baselines**
  - For each prompt:
    - FID(Generated[prompt] vs Real[prompt]).
    - IS(Generated[prompt]) vs IS(Real[prompt]).
    - SSIM between Generated[prompt] and nearest Real[prompt] (and SSIM between random real pairs as a baseline).
  - Global (all prompts combined):
    - FID(Generated[all] vs Real[all]).
  - Report 95% CIs via bootstrap for FID/IS to show statistical stability.

- **qualitative sanity checks**
  - Top-5 nearest neighbor gallery: for 10 random generated samples per prompt, show their nearest real images (by CLIP or Inception features).
  - Failure grid: 16 samples per prompt showing artifacts or prompt failures.

- **limitations to discuss**
  - FID depends on feature extractor; not sensitive to prompt faithfulness.
  - IS is calibrated to ImageNet classes; not ideal for domains outside ImageNet.
  - SSIM is structure/pixel-aligned; unfair unless paired by nearest neighbor and even then may penalize valid variations.
  - Dataset shift between real set and generative distribution affects all metrics.

### 3) Concrete execution steps

- **data prep**
  - Select 4 prompts and finalize stylistic modifiers.
  - Collect real images per prompt:
    - COCO: filter images by captions/labels for each object; sample N images per prompt; center-crop/resize to 512x512.
    - Alternative datasets: select N images per chosen classes, resize to 512x512.
  - Split real images into two halves to compute Real-vs-Real FID (upper bound / sanity check).

- **image generation**
  - For each prompt, generate N images at 512x512 with fixed steps/guidance and logged seeds.
  - Save images and a CSV/JSON log with prompt, seed, step, guidance, model hash, datetime.

- **feature extraction for metrics**
  - Use a single pipeline:
    - InceptionV3 features for FID/IS (e.g., `torchmetrics` or `clean-fid`).
    - For SSIM pairing: compute CLIP ViT-L/14 or Inception features for nearest neighbor search; then compute SSIM on paired images.

- **metric computation**
  - Per prompt and overall:
    - FID(Gen vs Real), IS(Gen) and IS(Real), SSIM(Gen, NN(Real)).
  - Bootstrapping (e.g., 500 resamples) to report 95% CIs for FID/IS.
  - Record runtime and throughput (images/sec) for method section.

- **analysis**
  - Tables: per-prompt FID/IS/SSIM with CIs; overall FID.
  - Plots: violin/bar for per-prompt FID; scatter of FID vs guidance scale if you run a tiny ablation (optional).
  - Qualitative panels: success/failure grids, NN galleries.
  - Discussion: metric contradictions (e.g., good FID but weak prompt faithfulness), sensitivity to image count, dataset mismatch.

### 4) Deliverables and structure

- **code submission (minimal, clean, reproducible)**
  - `env/` or instructions to set up environment.
  - `data/real/<prompt_slug>/...`, `data/gen/<prompt_slug>/...`
  - `configs/experiment.yaml` with prompts, counts, steps, guidance, seeds.
  - `scripts/generate.py` (produce images from config).
  - `scripts/compute_metrics.py` (FID/IS/SSIM; saves JSON results).
  - `notebooks/analysis.ipynb` (tables/plots).
  - Logs: one CSV per prompt with metadata.

- **slides (10–15 min)**
  - Intro to diffusion (diagram of denoise steps; key hyperparams).
  - Prompts, counts, generation settings.
  - Metrics (definitions, why each, limitations).
  - Results (tables/figures).
  - Challenges/limitations + what you’d do next.

### 5) Practical recommendations (to remove uncertainty)

- **where to apply**
  - Choose “object-centric, photo-real” for clean metrics and easy real-data matching (COCO-like categories or Flowers).
- **prompts**
  - Use the 4 prompts given above; keep style modifiers identical.
- **image counts**
  - If time-limited: 100 per prompt (400 total).
  - Recommended: 250 per prompt (1,000 total).
  - If you can batch overnight: 500 per prompt (2,000 total).
- **compute**
  - 512x512, 30 steps, guidance 7–7.5, sequential seeds.
  - Prefer local if you have GPU/MPS; otherwise batch via Replicate, track costs.

### 6) Timeline (assuming 1 week)

- Day 1: Lock domain, 4 prompts, counts; collect real dataset; finalize config.
- Day 2: Generate images for all prompts; log metadata.
- Day 3: Extract features; compute FID/IS; implement NN pairing for SSIM.
- Day 4: Bootstrap CIs; create tables/figures; assemble qualitative panels.
- Day 5: Slides; write discussion; sanity checks; dry run presentation.

### 7) Risks and mitigations

- **too few images → noisy FID/IS**: use at least 250 per prompt; report CIs; add overall FID across prompts.
- **dataset mismatch**: ensure real images match prompts; keep consistent preprocessing (resize/crop).
- **API variability**: fix seeds, prompts, and model version; log outputs; prefer local if possible.
- **SSIM pairing pitfalls**: always pair via nearest neighbor in feature space; also report random real-real SSIM baseline.

- Prompts and counts are now concrete, with clear metrics and comparisons. If you want, I can provide a minimal config file and the exact commands to run the generation and metric scripts next.

---

**User**

Ok, there were some great suggestions in there. Can you write in a more straight forward way? So we have to do x and then y and then z. Lets make it simple for the initial plan, only the basic to achieve a perfect score.

For instance, there are some steps I am not really sure: we are going to use the coco dataset? so that we have something to compare to? 

Lets focus more on the tasks itself.

---

**Cursor**

### Short, no-nonsense plan (do x, then y, then z)

- Are we using COCO?
  - No. We’ll use Food-101 (simpler, object-centric, easy real-vs-generated comparison). Classes: pizza, sushi, hamburger, cupcake.

### 1) Lock decisions (now)
- Model: `runwayml/stable-diffusion-v1-5` via `diffusers`.
- Resolution: 512x512. Steps: 30. Guidance: 7.5. Seeds: 0..249 per prompt.
- Prompts (4):
  - “a high-resolution food photography of a pizza, studio lighting, 50mm lens, realistic”
  - “a high-resolution food photography of sushi, studio lighting, 50mm lens, realistic”
  - “a high-resolution food photography of a hamburger, studio lighting, 50mm lens, realistic”
  - “a high-resolution food photography of a cupcake, studio lighting, 50mm lens, realistic”
- Negative prompt: “blurry, low-resolution, watermark, text”.
- Images per prompt: 250 (total 1,000). Match with 250 real images per class.

### 2) Prepare real data (Food-101)
- Download Food-101.
- For each class (pizza, sushi, hamburger, cupcake): sample 250 images (train split), center-crop and resize to 512x512.
- Save to `data/real/<class>/...`.

### 3) Generate images (diffusers)
- For each prompt: generate 250 images at 512x512 with seeds 0..249, 30 steps, guidance 7.5, shared negative prompt.
- Save to `data/gen/<class>/...`.
- Log `prompt, seed, steps, guidance, model_id, timestamp, file_path` to a CSV per class.

### 4) Compute metrics (at least two; we’ll do two cleanly)
- FID (per class and overall): `FID(Gen[class] vs Real[class])`, and `FID(Gen[all] vs Real[all])`.
- Inception Score (IS) on generated data (per class and overall). Optionally compute IS on real as an upper-bound reference.
- Output one JSON with all numbers.

### 5) Compare with real data
- Table with per-class FID and IS; add overall FID and IS.
- 2–3 sentences per metric:
  - What lower/higher means.
  - Which classes are better/worse and plausible reason (prompt fit, texture complexity).

### 6) Minimal qualitative checks
- For each class: grid of 16 generated samples.
- 1 page with 10 generated samples + their nearest real neighbors (CLIP embeddings) to sanity-check realism/prompt match.

### 7) Slides (10–15 minutes)
- 1: What diffusion is (one denoise diagram; 2 bullets on steps/guidance).
- 2: Our setup (model, prompts, counts, seeds, resolution).
- 3: Dataset (Food-101, 4 classes, preprocessing).
- 4: Metrics (FID, IS: definitions, what they capture).
- 5: Results table (per-class and overall).
- 6: Qualitative grids and nearest-neighbor panel.
- 7: Discussion: advantages/limitations of FID/IS and any failure cases.
- 8: Takeaways.

### 8) Submission
- Code + readme + results JSON + sample images + slides uploaded.
- Keep scripts minimal: one for generation, one for metrics. Config as a single YAML.

### 9) Time/cost controls
- If compute is tight, drop to 100 images/prompt (still acceptable). Keep settings fixed and report counts clearly.

### Deliverables checklist (acceptance criteria)
- 1,000 generated images across 4 prompts (or 400 if constrained), mirrored by 1,000 real images.
- One results JSON with per-class and overall FID and IS.
- Slides containing: model intro, prompts/settings, metrics, results, limitations, analysis.
- Reproducible seeds and logs.

- Chosen dataset: Food-101 (not COCO) to keep it simple, object-centric, and directly comparable. Prompts and image counts are fixed above.

---

**User**

Ok, that seems great! So a few questions:
1. What is this "Optionally compute IS on real as an upper-bound reference."?
2. In the CLIP part, how can we achieve this: "1 page with 10 generated samples + their nearest real neighbors (CLIP embeddings) to sanity-check realism/prompt match."? We simply compute the CLIP for each generated image and get the top-10?

---

**Cursor**

### 1) “Optionally compute IS on real as an upper-bound reference.”
- IS (Inception Score) is usually reported on generated images. To know what’s “good,” also compute IS on the matched real set (same classes, same preprocessing).
- Treat the real-set IS as a practical ceiling/baseline: if generated IS ≈ real IS, you’re very strong; if much lower, you’re weaker.
- Caveat: IS uses ImageNet classes; Food-101 isn’t ImageNet. Absolute values matter less than the gap Gen vs Real.
- While you’re at it, also compute Real-vs-Real FID (split real data in halves). That’s a practical lower bound for FID.

### 2) CLIP nearest-neighbor panel (10 generated → nearest real images)
- Goal: For 10 randomly chosen generated images, show each next to its top-k nearest real images using CLIP image embeddings.
- Simple steps:
  1) Embed all real images with CLIP; L2-normalize; store matrix R ∈ ℝ[n_real×d].
  2) Embed all generated images; L2-normalize; store G ∈ ℝ[n_gen×d].
  3) Pick 10 indices from G. For each selected embedding g, compute cosine similarity s = R · g and take top-k (e.g., k=3) indices.
  4) Build a grid: each row = [generated, NN1, NN2, NN3].
- “Top-10” can mean two things; what you want is: for each of the 10 chosen generated images, take its top-k neighbors from the real set (not the global top-10 generated).

Minimal code sketch (works with ~1k real images; no faiss needed):
```python
import os
from pathlib import Path
from PIL import Image
import torch
from transformers import CLIPModel, CLIPProcessor

def load_image_paths(dir_path: str) -> list[str]:
    exts = {".jpg", ".jpeg", ".png", ".webp"}
    return [str(p) for p in Path(dir_path).rglob("*") if p.suffix.lower() in exts]

def load_clip(model_id: str = "openai/clip-vit-large-patch14", device: str = "cuda" if torch.cuda.is_available() else "cpu") -> tuple[CLIPModel, CLIPProcessor, str]:
    model = CLIPModel.from_pretrained(model_id).to(device).eval()
    processor = CLIPProcessor.from_pretrained(model_id)
    return model, processor, device

def embed_images(paths: list[str], model: CLIPModel, processor: CLIPProcessor, device: str, batch_size: int = 32) -> torch.Tensor:
    feats: list[torch.Tensor] = []
    for i in range(0, len(paths), batch_size):
        batch = [Image.open(p).convert("RGB") for p in paths[i:i+batch_size]]
        inputs = processor(images=batch, return_tensors="pt").to(device)
        with torch.no_grad():
            emb = model.get_image_features(**inputs)  # (B, d)
        feats.append(emb.cpu())
    X = torch.cat(feats, dim=0)  # (N, d)
    X = X / X.norm(dim=1, keepdim=True)
    return X  # cosine-ready

def topk_neighbors(query_vecs: torch.Tensor, base_vecs: torch.Tensor, k: int) -> tuple[torch.Tensor, torch.Tensor]:
    # query_vecs: (Q, d), base_vecs: (N, d) — both L2-normalized
    sims = query_vecs @ base_vecs.T  # (Q, N)
    vals, idxs = sims.topk(k=k, dim=1)
    return vals, idxs  # cosine similarities and indices

# Example usage:
# real_dir = "data/real/pizza + sushi + hamburger + cupcake (combined or per-class)"
# gen_dir  = "data/gen/pizza + sushi + hamburger + cupcake"
# Better: do per-class, and pick 10 generated within each class for class-specific panels.

# model, proc, dev = load_clip()
# real_paths = load_image_paths(real_dir)
# gen_paths  = load_image_paths(gen_dir)
# R = embed_images(real_paths, model, proc, dev)  # (N_real, d)
# G = embed_images(gen_paths, model, proc, dev)   # (N_gen, d)
# sel = torch.randint(low=0, high=G.size(0), size=(10,))
# vals, idxs = topk_neighbors(G[sel], R, k=3)
# For panel building: for row j in range(10), use gen_paths[sel[j]] + [real_paths[i] for i in idxs[j].tolist()]
```

- Distance choice: Use cosine (after L2-normalization). Equivalent to Euclidean in normalized space.
- If the real set is large (>50k), switch to FAISS; otherwise brute-force is fine.

Summary:
- IS on real = a reference ceiling to compare against generated IS; report both (and Real-vs-Real FID).
- CLIP panel = per generated image, retrieve its top-k nearest real neighbors via cosine similarity on CLIP image embeddings; show 10 such rows.

```python
import os
from pathlib import Path
from PIL import Image
import torch
from transformers import CLIPModel, CLIPProcessor

def load_image_paths(dir_path: str) -> list[str]:
    exts = {".jpg", ".jpeg", ".png", ".webp"}
    return [str(p) for p in Path(dir_path).rglob("*") if p.suffix.lower() in exts]

def load_clip(model_id: str = "openai/clip-vit-large-patch14", device: str = "cuda" if torch.cuda.is_available() else "cpu") -> tuple[CLIPModel, CLIPProcessor, str]:
    model = CLIPModel.from_pretrained(model_id).to(device).eval()
    processor = CLIPProcessor.from_pretrained(model_id)
    return model, processor, device

def embed_images(paths: list[str], model: CLIPModel, processor: CLIPProcessor, device: str, batch_size: int = 32) -> torch.Tensor:
    feats: list[torch.Tensor] = []
    for i in range(0, len(paths), batch_size):
        batch = [Image.open(p).convert("RGB") for p in paths[i:i+batch_size]]
        inputs = processor(images=batch, return_tensors="pt").to(device)
        with torch.no_grad():
            emb = model.get_image_features(**inputs)  # (B, d)
        feats.append(emb.cpu())
    X = torch.cat(feats, dim=0)  # (N, d)
    X = X / X.norm(dim=1, keepdim=True)
    return X  # cosine-ready

def topk_neighbors(query_vecs: torch.Tensor, base_vecs: torch.Tensor, k: int) -> tuple[torch.Tensor, torch.Tensor]:
    # query_vecs: (Q, d), base_vecs: (N, d) — both L2-normalized
    sims = query_vecs @ base_vecs.T  # (Q, N)
    vals, idxs = sims.topk(k=k, dim=1)
    return vals, idxs  # cosine similarities and indices

# Example usage:
# real_dir = "data/real/pizza + sushi + hamburger + cupcake (combined or per-class)"
# gen_dir  = "data/gen/pizza + sushi + hamburger + cupcake"
# Better: do per-class, and pick 10 generated within each class for class-specific panels.

# model, proc, dev = load_clip()
# real_paths = load_image_paths(real_dir)
# gen_paths  = load_image_paths(gen_dir)
# R = embed_images(real_paths, model, proc, dev)  # (N_real, d)
# G = embed_images(gen_paths, model, proc, dev)   # (N_gen, d)
# sel = torch.randint(low=0, high=G.size(0), size=(10,))
# vals, idxs = topk_neighbors(G[sel], R, k=3)
# For panel building: for row j in range(10), use gen_paths[sel[j]] + [real_paths[i] for i in idxs[j].tolist()]
```

---

**User**

Ok, so in the end the CLIP step is to select 10 random images and get the top-k neighbor from the real images, correct?

---

**Cursor**

### Yes — that’s exactly it

- Select 10 random generated images (ideally per class).
- Compute CLIP image embeddings for all real and generated images once; L2-normalize.
- For each of the 10 generated embeddings, compute cosine similarity against the real-set embeddings (prefer same class).
- Take top-k neighbors (e.g., k=3) and build a panel: [generated | NN1 | NN2 | NN3].
- Fix a random seed for reproducibility.

- In short: pick 10 generated samples, retrieve their top-k nearest real neighbors via CLIP cosine similarity, and show them side-by-side.

---

**User**

Awesome, so now coming back to one of the most important steps of the assignment. Can you show me how can I use a diffusion model? I have a macbook pro M2 so I have a fair amount of compute.

---

**Cursor**

### Minimal, working setup on an M2 Mac (local, fast, reproducible)

- Install deps and authenticate once (for Stable Diffusion model access):
```bash
uv run --with huggingface_hub[cli] huggingface-cli login
```
- Generate images with this script (MPS-accelerated on Apple Silicon). Save it as `scripts/generate.py` and run it.

```python
from __future__ import annotations

from pathlib import Path
from datetime import datetime
import csv

import torch
from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler


MODEL_ID: str = "runwayml/stable-diffusion-v1-5"  # accept license on the model page first
NEGATIVE_PROMPT: str = "blurry, low-resolution, watermark, text"
PROMPTS: dict[str, str] = {
    "pizza": "a high-resolution food photography of a pizza, studio lighting, 50mm lens, realistic",
    "sushi": "a high-resolution food photography of sushi, studio lighting, 50mm lens, realistic",
    "hamburger": "a high-resolution food photography of a hamburger, studio lighting, 50mm lens, realistic",
    "cupcake": "a high-resolution food photography of a cupcake, studio lighting, 50mm lens, realistic",
}
IMAGES_PER_PROMPT: int = 250  # use 2–5 for a smoke test first
HEIGHT: int = 512
WIDTH: int = 512
STEPS: int = 30
GUIDANCE: float = 7.5
OUT_DIR: Path = Path("data/gen")


def setup_pipe(model_id: str, device: str) -> StableDiffusionPipeline:
    dtype = torch.float16 if device == "mps" else torch.float32
    pipe = StableDiffusionPipeline.from_pretrained(
        model_id,
        torch_dtype=dtype,
        use_safetensors=True,
        variant="fp16" if dtype == torch.float16 else None,
    )
    pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
    pipe = pipe.to(device)
    pipe.set_progress_bar_config(disable=False)
    return pipe


def ensure_dir(path: Path) -> None:
    path.mkdir(parents=True, exist_ok=True)


def generate_images(
    model_id: str,
    prompts: dict[str, str],
    images_per_prompt: int,
    out_dir: Path,
    negative_prompt: str,
    height: int,
    width: int,
    steps: int,
    guidance: float,
) -> None:
    device = "mps" if torch.backends.mps.is_available() else "cpu"
    pipe = setup_pipe(model_id, device)
    ensure_dir(out_dir)

    meta_rows: list[list[str]] = [["file_path", "label", "prompt", "seed", "steps", "guidance", "model_id", "timestamp"]]

    for label, prompt in prompts.items():
        class_dir = out_dir / label
        ensure_dir(class_dir)

        for seed in range(images_per_prompt):
            generator = torch.Generator(device=device).manual_seed(seed)
            image = pipe(
                prompt=prompt,
                negative_prompt=negative_prompt,
                num_inference_steps=steps,
                guidance_scale=guidance,
                height=height,
                width=width,
                generator=generator,
            ).images[0]

            filename = f"{label}_{seed:04d}.png"
            filepath = class_dir / filename
            image.save(filepath)

            meta_rows.append([
                str(filepath),
                label,
                prompt,
                str(seed),
                str(steps),
                f"{guidance:.2f}",
                model_id,
                datetime.utcnow().isoformat(timespec="seconds") + "Z",
            ])

    with (out_dir / "gen_metadata.csv").open("w", newline="") as f:
        csv.writer(f).writerows(meta_rows)


if __name__ == "__main__":
    generate_images(
        model_id=MODEL_ID,
        prompts=PROMPTS,
        images_per_prompt=IMAGES_PER_PROMPT,
        out_dir=OUT_DIR,
        negative_prompt=NEGATIVE_PROMPT,
        height=HEIGHT,
        width=WIDTH,
        steps=STEPS,
        guidance=GUIDANCE,
    )
```

Run it:
```bash
# quick smoke test (override deps ad-hoc)
uv run --with torch --with torchvision --with diffusers --with transformers --with accelerate --with safetensors scripts/generate.py
```

Notes:
- If you get a 403 when loading the model, visit the model page, accept the license, then rerun after `huggingface-cli login`.
- For a fast dry-run: set `IMAGES_PER_PROMPT = 2`. Once it works, set it back to 250.
- Output goes to `data/gen/<class>/*.png` plus `data/gen/gen_metadata.csv`.

Optional speed toggle:
- For quick drafts (lower quality, much faster), switch `MODEL_ID` to `stabilityai/sd-turbo` and set `STEPS = 4`.

```shellscript
uv run --with huggingface_hub[cli] huggingface-cli login
```

```python
from __future__ import annotations

from pathlib import Path
from datetime import datetime
import csv

import torch
from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler


MODEL_ID: str = "runwayml/stable-diffusion-v1-5"  # accept license on the model page first
NEGATIVE_PROMPT: str = "blurry, low-resolution, watermark, text"
PROMPTS: dict[str, str] = {
    "pizza": "a high-resolution food photography of a pizza, studio lighting, 50mm lens, realistic",
    "sushi": "a high-resolution food photography of sushi, studio lighting, 50mm lens, realistic",
    "hamburger": "a high-resolution food photography of a hamburger, studio lighting, 50mm lens, realistic",
    "cupcake": "a high-resolution food photography of a cupcake, studio lighting, 50mm lens, realistic",
}
IMAGES_PER_PROMPT: int = 250  # use 2–5 for a smoke test first
HEIGHT: int = 512
WIDTH: int = 512
STEPS: int = 30
GUIDANCE: float = 7.5
OUT_DIR: Path = Path("data/gen")


def setup_pipe(model_id: str, device: str) -> StableDiffusionPipeline:
    dtype = torch.float16 if device == "mps" else torch.float32
    pipe = StableDiffusionPipeline.from_pretrained(
        model_id,
        torch_dtype=dtype,
        use_safetensors=True,
        variant="fp16" if dtype == torch.float16 else None,
    )
    pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
    pipe = pipe.to(device)
    pipe.set_progress_bar_config(disable=False)
    return pipe


def ensure_dir(path: Path) -> None:
    path.mkdir(parents=True, exist_ok=True)


def generate_images(
    model_id: str,
    prompts: dict[str, str],
    images_per_prompt: int,
    out_dir: Path,
    negative_prompt: str,
    height: int,
    width: int,
    steps: int,
    guidance: float,
) -> None:
    device = "mps" if torch.backends.mps.is_available() else "cpu"
    pipe = setup_pipe(model_id, device)
    ensure_dir(out_dir)

    meta_rows: list[list[str]] = [["file_path", "label", "prompt", "seed", "steps", "guidance", "model_id", "timestamp"]]

    for label, prompt in prompts.items():
        class_dir = out_dir / label
        ensure_dir(class_dir)

        for seed in range(images_per_prompt):
            generator = torch.Generator(device=device).manual_seed(seed)
            image = pipe(
                prompt=prompt,
                negative_prompt=negative_prompt,
                num_inference_steps=steps,
                guidance_scale=guidance,
                height=height,
                width=width,
                generator=generator,
            ).images[0]

            filename = f"{label}_{seed:04d}.png"
            filepath = class_dir / filename
            image.save(filepath)

            meta_rows.append([
                str(filepath),
                label,
                prompt,
                str(seed),
                str(steps),
                f"{guidance:.2f}",
                model_id,
                datetime.utcnow().isoformat(timespec="seconds") + "Z",
            ])

    with (out_dir / "gen_metadata.csv").open("w", newline="") as f:
        csv.writer(f).writerows(meta_rows)


if __name__ == "__main__":
    generate_images(
        model_id=MODEL_ID,
        prompts=PROMPTS,
        images_per_prompt=IMAGES_PER_PROMPT,
        out_dir=OUT_DIR,
        negative_prompt=NEGATIVE_PROMPT,
        height=HEIGHT,
        width=WIDTH,
        steps=STEPS,
        guidance=GUIDANCE,
    )
```

```shellscript
# quick smoke test (override deps ad-hoc)
uv run --with torch --with torchvision --with diffusers --with transformers --with accelerate --with safetensors scripts/generate.py
```

